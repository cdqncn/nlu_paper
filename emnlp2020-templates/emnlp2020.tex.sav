%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{makecell}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for EMNLP 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020.
The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
These instructions should be used for both papers submitted for review and for final versions of accepted papers.
Authors are asked to conform to all the directions reported in this document.
\end{abstract}

\section{Credits}

This document has been adapted by Yulan He
from the instructions for earlier ACL and NAACL proceedings, including those for
ACL 2020 by Steven Bethard, Ryan Cotterrell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Michell and Stephanie Lukin,
2017/2018 (NA)ACL bibtex suggestions from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan,
NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shing Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

\section{Introduction}

Spoken language understanding is playing an important part of the task-based dialogue system, which can identify the intent and extract the corresponding slots form user's utterances. Therefore, SLU consists of two subtasks: intent detection and slot filling(Tur and De Mori, 2011)\citep{1}. For example, as shown in Table 1, the intent of the sentence ``from seattle to salt lake city '' is ``atis\_flight '', and each word corresponds to a slot label.

\begin{table}
\centering
\tiny
\renewcommand\tabcolsep{2.0pt}
\begin{tabular}{llllll}
\hline \textbf{from} & \textbf{seattle} & \textbf{to} & \textbf{salt}& \textbf{lake}& \textbf{city}\\ \hline
O  & B-fromloc.city\_name & O & B-toloc.city\_name & I-toloc.city\_name & I-toloc.city\_name \\
atis\_flight \\
\hline
\end{tabular}
\caption{\label{font-table} Font guide. }
\end{table}

The traditional method uses pipline to solve these two tasks. Specifically, intent detection is a separate classification task, and slot extraction is a separate sequence labeling task, but there is a strong correlation between these two tasks (Goo et al., 2018)\citep{2}. For example, the intent of the sentence ``from seattle to salt lake city'' is ``flight'', so the slot label of the word ``seattle'' is more inclined to ``fromloc.city\_name'' rather than ``city\_name''. Conversely, when the slot labels for the word ``seattle'' and the phrase ``salt lake city'' are ``fromloc.city\_name'' and ``toloc.city\_name'', the intention of the discourse is most likely ``flight''. Due to the correlation between these two tasks, some work uses joint modeling to explore the connection between them (Zhang and Wang, 2016; Hakkani-Tur et al., 2016; Liu and Lane, 2016)\citep{3,4,5}.

Benefiting from the advantages of parameter sharing brought by multi-task learning, these models are superior to the pipline-based method. However, these works did not further explore the relationship between intent detection and slot filling. Goo et al.(2018)\citet{2} and Li et al.(2018)\citet{6} use the gating mechanism to selectively add intent information to the slot filling task.

Qin et al.(2019)\citet{7} proposed a joint model with Stack-Propagation for SLU tasks, which will identify the intent of each token and input the intent representation of each token into the slot filling task. If the intent of some tokens is predicted incorrectly, the intent of other tokens will also be useful for the slot filling of each token. Wang et al.(2018)\citet{8} established two task networks for the intent detection task and the slot filling task. The two task networks are iterative during the training process, and the prediction results of the other task will be used when training one task.

Although these works have achieved good results on SLU tasks, they have not modeled the interaction between intent and slot during a training process. Goo et al.(2018)\citet{2}, Li et al.(2018)\citet{6} and Qin et al.(2019)\citet{7} are to explore how to better integrate the intent information into the slot extraction task. Although work D considers inputting the results of slot filling into the intent detection task, iterative training cannot model the interaction between them. How to model the interaction of two tasks faces two challenges: 1.When the input is just a sentence, how to get an explicit representation of intent and position, it is the basis for modeling the interaction between intent and slot. 2.How to better model the information interaction between intent and slot.

In this paper,  we propose A Collaborative-Transformer to solve the above two challenges.

\begin{itemize}
\item First, inspired by the lan model, we introduced intent label embeding and slot label embedding and designed the intent label attention layer and slot label attention layer. In this way, we can explicitly get the intent representation and slot representation.
\item Second, we designed an interactive transfomer block, which can complete the two-way information interaction between the intent and the slot inside the encoder. And the stacking of multi-layer interactive transfomer blocks allows intent representation and slot representation to deeply integrate the information of each other to improve the performance of the two tasks.
\end{itemize}

We conduct experiments on two benchmark datasets of ATIS and SNIPS. Experimental results show that the performance of our model exceeds the current state-of-the-art in multiple evaluation indicators.

The contributions of our work are as follows:

1)We propose an A Collaborative-Transformer joint model, which can not only use intent knowledge to improve the performance of slot filling tasks, but also use slot knowledge to guide the detection of intent, and the information interaction between intent and slot is simultaneous.

2)We constructed intent label attention and slot label attnetion to explicitly obtain the representation of intent and slot, which established a foundation for the interaction of information between intent and slot in the interactive trasnfomer block.

3)The experimental results on two public data sets show the superiority of our model in performance, and our model has achieved state-of-the-art on multiple indicators.

The main idea is to design a special interactive transfomer structure to complete the information exchange and fusion between intent detection and slot filling.
\section{Related Work}
The intent detection task is regarded as a sentence classification task. At present, many text classification methods are used to solve this task, such as support vector machine (SVM), recurrent neural network (RNN), convolutional neural network (CNN) (Haffner et al., 2003; Sarikaya et al. , 2011; Hashemi et al., 2016). The slot filling task is solved as a sequence labeling task. Common methods are conditional random field (CRF), recurrent neural network (RNN), and LSTM-CRF (Huang et al. 2015). Cui et al. (2019) Proposed LAN instead of CRF to capture the dependencies between labels.

In recent years, some methods based on joint training have avoided the error propagation of the pipline method. Zhang and Wang (2016)first uses the GRU network for the slu task, and the intent detection and slot filling share the same GRU encoder.  Liu and Lane (2016)proposed an attention-based encoder-decoder joint model for the two task. Benefiting from parameter sharing and joint training between the two tasks, these models perform better than the pipline model. However, these methods only rely on parameter sharing to establish the connection between the two tasks, and do not specifically model the representation of the slot or the representation of intent.

Recently, some work has begun to explore the enhancement of slot filling tasks by intent information. Goo et al. (2018) designed slot-gate to control the flow of information between intent filling and slot filling tasks. Li et al. (2018) proposed an intention enhancement gate mechanism to capture the dependency between intent and slot.  Qin et al. (2019) proposed the Stack-Propagation framework to predict the intent of each token and pass it to the slot decoder, and in this way, the model can avoid slot filling errors caused by incorrect prediction of the overall intent of utence. These models only model the flow of intent information to slot filling tasks, and do not pay attention to the impact of slot information on intent detection. Wang et al. (2018) proposed Bi-model to iteratively enhance the other task with the result of one task during the training process. Zhang et al. (2019) propose a hierarchicalcapsule neural network to model the the hierarchi cal relationship among word, slot, and intent in an utterance. E et al. (2019) introduced an SF-ID network, which includes SF sub-network and ID sub-network. The two sub-networks iteratively achieve the flow of information between intent and slot. This limits their performance and makes these networks unable to adequately model the complex interactive information between intents and slots. However, in these networks, information is only transferred from intent to slot, and then from slot to intent. The information interaction between intent and slot is not completed at the same time, which limits their performance and make these networks unable to adequately model the complex interactive information between intent and slot. Compared with these models, in our proposed co-transfomer network, intent representation and slot representation can be obtained explicitly and interact at the same time. In this way, our model can better establish the correlation between intent and slot at the word level and semantic level.

 Compared with these models, in our proposed co-transfomer network, intent representation and slot representation can interact at the same time, and the token level and semantic level association between these two representations will also be captured, such as the correlation between intent word and slot value.

\section{Proposed model }

In this section, we will introduce each subnetwork of the Collaborative-Transformer model in detail. First, we use a BiLstm as a shared encoder to obtain the context sequence information of the input sentence, which can capture the shared features between the intent detection task and the slot filling task. Next we designed stacked co-tranfomer blocks to model the interaction between intents and slots. Finally, the intent representation and slot representation of the co-transfomer block output of the last layer are input to the softmax layer and the CRF layer respectively for intent detection and slot filling. We use joint learning to train the model.

\subsection{Shared encoder}
LSTM is widely used in classification tasks and sequence labeling tasks, and as a baseline model for these tasks. We choose BILSTM as the shared encoder layer of our model. The output of each utterance word embedding layer is E = (e1, e2, ..., et). After input to BiLSTM, we can get the hidden state of this utterance H = (h1, h2, ..., ht). As a shared representation of intent and slot, H can provide context information and time series information for Co-Transfomer block, which are useful for slot filling tasks.
\subsection{Co-tranfomer}
Co-tranfomer block is the main innovation of this paper. Each co-tranfomer block includes three networks: intent-slot label attention layer; interactive attention layer for intent representation and slot representation, and a feed-forward network layer for information fusion.
\subsection{Intent-slot label attention layer}
In order to get the intent representation and slot representation explicitly, before interactive attention, we created the intent label attention layer and slot label attention layer. Specifically, we use the parameters of the output layer of the slot filling task as slot embedding Semb. The parameter matrix W of the output layer can be regarded as the distribution of labels in a certain sense. For example, in the intent detection task, without considering the bias, the final score of each category is actually the final representation of the intent and the W in each. The inner product of a column (the inner product can be regarded as a similarity calculation), the index corresponding to the column with the highest score is the prediction category. So we treat W as a label embedding. Then we use the hidden state H as the query, Semb as the key and value, and then obtain the slot representation through the slot label attention layer. The attention calculation process is as follows£º

\begin{gather}
	A=softmax(HS^T_{emb}) \\
	H_S=H+AS^T_{emb}
\end{gather}

 There is no additional parameter in the integration process. and the intent attention layer is the same as slot attention layer. Therefore, we can get the intent representation HI and the slot representation $H_S$.  The role of the label attention layer in the first layer of co-transfomer block is to explicitly obtain HI and $H_S$, and the label attention layer in the subsequent N-1 layer is mainly for the interaction between the label and the representation.

\subsection{Collaborative attention layer}
In our co-transfomer block, the main role of the collaborative attention layer is the information interaction of intent and slot, which is simultaneous and symmetrical. Intent knowledge and slot knowledge are integrated into intent representation and slot representation respectively, so we can get a slot-aware intent representation and a slot-aware intent representation. The input of the co-attention layer is the intent representation Hi=(h1,h2,..,h3) and slot representation  H\_S=(h1, h2,...h3) generated by the label attention layer.

\begin{gather}
	Q_I,K_I,V_I=H_{I}W^Q_I,H_{S}W^K_I,H_SW^V_I \\
    Q_S,K_S,V_S=K_{I}^T,H_S,H_S,W^V_S \\
	A_I=softmax \frac{(Q_IK^T_I)}{\sqrt{d_k}}\\
    A_S=A^T_I=softmax \frac{(Q_SK^T_S)}{\sqrt{d_k}}\\
    =softmax \frac{(Q_IK^T_I)^T}{\sqrt{d_k}}\\
    H_I^{NEW}=H_I+A_IV_I\\
    H_S^{NEW}=H_S+A_SV_S
\end{gather}

Here $Q_I, K_I, V_I$ are intent query, key and value, $Q_S, K_S, V_S$ represent $W_I^Q$ query, key and value of slot, $W_I^K, W_I^K, W_S^V$ are weight matrix corresponding to $Q_I, K_I, V_I, V_S, Q_I and K_I$ are the transpose of $K_S$ and $Q_S$, and Ai is the transpose of As.

In fact, the attention matrix $A_I$ is generated by the inner product of intent query $Q_I (K_S)$ and slot query $Q_S (K_S)$, and the collaborative attention is symmetrical about intent and slot. In this way, the intent detection task and the slot filling task complete the knowledge interaction and establish a token-level connection.

\subsection{Feed-forward network layer for information fusion}

In the previous section, we explicitly modeled the interaction between intent and slot through co-attention, and got the new intent representation Hinew = () and slot representation Hsnew = (). In this section, we use feed-forward network layer to implicitly merge intent knowledge and slot knowledge.

First, we concatenate $H_I$ and $H_S$ as $r_{is}$, and then we concatenate each $r_{is}^t$ of $r_{is}$ with $r_{is}^{t-1}$ and $r_{is}^{t+1}$ as $r^t$ :

\begin{gather}
	r_{is}=H_I^{new} \oplus H_S^{new}\\
    r^t=r_{is}^{t-1}\oplus r_{is}^{t} \oplus r_{is}^{t+1}
\end{gather}

where $\oplus$ is concatenation operation.

Next, the FFN layer fuses the window features, intent features and slot features contained in $r$.

\begin{gather}
	FFN(x)=max(0,xW_1+b_1)W_2+b_2\\
    O_I =H_I^{new}+FFN(r)
    O_S =H_S^{new}+FFN(r)
\end{gather}

The fusion of intent and slot in the FFN layer is equivalent to the implicit interaction between intent and slot. The model automatically integrates intent knowledge and slot knowledge, adn window features are very useful for slot extraction tasks (wang: 2016 gru).

\subsection{Joint Training}

The output of the last layer of the co-transfomer block, $O_I = O_S$, serves as the final representation of intent and slot. For the intent detection task, $O_I$ is used for intent prediction after the output layer is mapped:

\begin{gather}
	v_I=maxpool(O_I^{final})\\
    y_{I\_pred}=softmax(W_Iv_I+b_I)
\end{gather}

Where y is the predicted distribution of intent and the loss function is formulated as:

\begin{gather}
L_{inte}=\sum{y^i_{I\_label}\log(y^i_{I\_label})}
\end{gather}

For the slot filling task, to take advantage of the global dependencies between labels we use CRF to search for the global optimal prediction sequence. $O_S$ is first mapped to the label distribution $Y_S$ by the output layer:

\begin{gather}
Y_S=Wv_I+b_I
\end{gather}

Then we input $Y_S$ into the CRF, and use the Viterbi algorithm to decode the prediction sequence Ypred. Given a sequence s = [s1, s2, ..., sT ], the corresponding golden label sequence is y = [y1, y2, ..., yT ],  the loss function is formulated as:

\begin{gather}
L_{slot}=\log( p(y_{S\_label}\mid x))\\
=-(s(x,y_{S\_label})-\log(\sum_{ \widetilde y\in Y_X}e^{s(x, \widetilde y)})
\end{gather}

Where s represents the score of a path, $Yx$ is all possible label sequences, S(.) represents the score of one path.
We use joint training to optimize these two tasks simultaneously. Compared with the pipline model, the joint training method can effectively reduce the error propagation.(Zhang and Wang, 2016). the fifinal joint objective is formulated as :
\begin{gather}
L= \lambda L_{intent} + (1-\lambda)L_{slot}
\end{gather}

Where A is the hyperparameter combining the two losses


\section*{Acknowledgments}

The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
Do not include this section when submitting your paper for review.

\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}



\end{document}
